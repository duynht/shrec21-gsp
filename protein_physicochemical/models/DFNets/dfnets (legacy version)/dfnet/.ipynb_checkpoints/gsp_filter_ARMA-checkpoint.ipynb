{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.optimize import lsq_linear\n",
    "import scipy.sparse as sp\n",
    "from cvxpy import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agsp_filter_ARMA(M, b, a, x, T, tol=1e-4):\n",
    "    \n",
    "    '''\n",
    "    FILTER_ARMA Simulates a direct ARMA graph filter under a static signal and graph. \n",
    "    \n",
    "    INPUT ARGUMENTS\n",
    "      M:   This is the shifted Laplacian matrix M = 0.5*norm(L)I - L,\n",
    "           where L is the combinatorial/normalized Laplacian. \n",
    "           (Shifting the Laplacian helps the stability.) \n",
    "           \n",
    "      x:   The graph signal to be filtered.\n",
    "    \n",
    "      b,a: are the coefficient of the numerator and denominator polynomials \n",
    "           of the filter's spectral response:\n",
    "           $$r(mu) = \\frac{ sum_{j=0}^{K-1} b(j+1) mu^{j} }{ sum_{j=0}^{K}\n",
    "           a(j+1) mu^{j} }$$.\n",
    "           Note that mu here should be the eigenvalues of M. This means that\n",
    "           since M = 0.5*norm(L)I - L, the polynomials should also\n",
    "           be expressed in terms of the eigenvalues mu of M.\n",
    "    \n",
    "     T:    How many iterations should the ARMA recursions be run for?\n",
    "    \n",
    "     verbose: Set to 1 to see more information, 0 otherwize\n",
    "    \n",
    "    OUTPUT ARGUMENTS\n",
    "    \n",
    "     y:    The filtered signal for all iterations\n",
    "    \n",
    "    EXAMPLE USAGE\n",
    "    \n",
    "    Suppose we want to filter a signal x on a path graph G\n",
    "        G = gsp_path(100);\n",
    "        G = gsp_create_laplacian(G, 'normalized');\n",
    "        x = rand(G.N,1);\n",
    "    \n",
    "    To approximate r(lambda) = tau / (tau + lambda), where lambda is an \n",
    "    eigenvalue of the normalized Laplacian L: \n",
    "        M = eye(G.N) - G.L;\n",
    "        tau = 1; T = 10;\n",
    "        y = agsp_filter_ARMA(M, x, [tau], [tau+1, -1], T, 1);\n",
    "    \n",
    "    We can vizualize the filtering output for each iterations as follows: \n",
    "        figure; plot(y')\n",
    "    '''\n",
    "    \n",
    "    a = a / a[1]\n",
    "    b = b / b[1]\n",
    "    \n",
    "    n = M.shape[0]\n",
    "    Ka = np.size(a)-1\n",
    "    Kb = np.size(b)-1\n",
    "    \n",
    "    y = np.zeros((n,T))\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        y[:, t] = 0\n",
    "        \n",
    "        #AR terms\n",
    "        for k in range(0, Ka):\n",
    "            if(t-1) > 0:\n",
    "                if(k==1):\n",
    "                    z = y[:, t-1]\n",
    "                z = M * z\n",
    "                y[:, t] = y[:, t] - a[k+1] * z\n",
    "                \n",
    "        #MA terms\n",
    "        z = x\n",
    "        for k in range(0, Kb):\n",
    "            #print((b[k+1]).shape)\n",
    "            #print((y[:, t].reshape((y[:, t]).shape[0], 1)).shape)\n",
    "            #print((b[k+1] * z).shape)\n",
    "            y[:, t] = y[:, t].reshape((y[:, t]).shape[0], 1).conj().transpose() - b[k+1] * z\n",
    "            z = M * z\n",
    "        \n",
    "        norm_difference = np.linalg.norm(y[:, t] - y[:, t-1])/np.linalg.norm(y[:, t-1])\n",
    "        if(t > 1 and norm_difference < tol):\n",
    "            break\n",
    "            \n",
    "    y = y[:, 0:t]\n",
    "    \n",
    "    return y\n",
    "\n",
    "def agsp_filter_ARMA_cgrad(L, b, a, x, Tmax, tol=1e-10):\n",
    "    \n",
    "    '''\n",
    "    AGSP_FILTER_ARMA_CGRAD Efficient implementation of an ARMA graph filter\n",
    "     \n",
    "    INPUT ARGUMENTS\n",
    "      L:   This is the Laplacian matrix.\n",
    "           \n",
    "      x:   The graph signal to be filtered.\n",
    "    \n",
    "      b,a: are the coefficient of the numerator and denominator polynomials \n",
    "           of the filter's spectral response:\n",
    "           $$r(lambda) = \\frac{ sum_{j=0}^{K-1} b(j+1) lambda^{j} }{ sum_{j=0}^{K}\n",
    "           a(j+1) lambda^{j} }$$.\n",
    "           Note that the polynomial coefficients (AR/MA) should be provided in\n",
    "           increasing power form contrary to matlab's convention. For instance, here \n",
    "           a(0) is the coefficient of L^0. \n",
    "     \n",
    "    OPTIONAL INPUTS\n",
    "    \n",
    "      tol:  is the tolerance of the iteration\n",
    "    \n",
    "      y0:   is an initial guess for the solution\n",
    "    \n",
    "      Tmax: is the maximum number of iterations\n",
    "    \n",
    "    OUTPUT ARGUMENTS\n",
    "    \n",
    "      y:    The filtered signal for all iterations\n",
    "    \n",
    "    EXAMPLE USAGE\n",
    "    \n",
    "    Suppose we want to filter a signal x on a path graph G\n",
    "        G = gsp_path(100);\n",
    "        G = gsp_create_laplacian(G, 'normalized');\n",
    "        x = rand(G.N,1);\n",
    "    \n",
    "    To approximate r(lambda) = tau / (tau + lambda), where lambda is an \n",
    "    eigenvalue of the normalized Laplacian L: \n",
    "        tau = 1; Tmax = 10;\n",
    "        y = agsp_filter_ARMA_cgrad(M, [tau], [tau, 1], c)\n",
    "    '''\n",
    "    \n",
    "    #Tmax = L.shape[0]\n",
    "    \n",
    "    # sparse polynomial multiplication \n",
    "    def L_mult(coef, x):\n",
    "        y = coef[0] * x\n",
    "        for i in range(1, len(coef)):\n",
    "            x = L * x\n",
    "            y = y + coef[i] * x\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    b = L_mult(b, x)\n",
    "\n",
    "    # initialization \n",
    "    y0 = b\n",
    "\n",
    "    y = y0\n",
    "    r = b - L_mult(a, y)\n",
    "    p = r\n",
    "    rsold = (r.conj().transpose())*r\n",
    "    \n",
    "    for k in range(0, Tmax):\n",
    "\n",
    "        Ap = L_mult(a, p) \n",
    "        alpha = rsold /(p.conj().transpose() * Ap)\n",
    "\n",
    "        y = y + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rsnew = (r.conj().transpose())*r\n",
    "\n",
    "        if(np.sqrt(rsnew).all() <= tol):\n",
    "            break\n",
    "\n",
    "        p = r + (rsnew/rsold)*p\n",
    "        rsold = rsnew\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmax(L, normalized=True):\n",
    "    \"\"\"Upper-bound on the spectrum.\"\"\"\n",
    "    if normalized:\n",
    "        return 2\n",
    "    else:\n",
    "        return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agsp_design_ARMA(mu, response, Kb, Ka, radius=0.85, show=0):\n",
    "    \n",
    "    '''\n",
    "    AGSP_DESIGN_ARMA This function finds polynomial coefficients (b,a) such that \n",
    "    the ARMA model \"rARMA = polyval(wrev(b),mu)./polyval(wrev(a), mu)\" \n",
    "    approximates the function response() at the points mu.\n",
    "     \n",
    "    REQUIRED INPUTS\n",
    "    mu the points where the response function is evaluated \n",
    "    responce is the desired response function \n",
    "    Kb,Ka are the orders of the numberator and denominator, respectively \n",
    "    \n",
    "    OPTIONAL INPUTS\n",
    "    radius allows one to control the tradeoff between convergence speed (small)\n",
    "        and accuracy (large). Should be below 1 if the standard iterative \n",
    "        implementation is used. With the conj. gradient implementation any \n",
    "        radius is allowed. \n",
    "    show set to 1 in order to display the approximation result\n",
    "    \n",
    "    Note that the polynomial coefficients (b/a) are returned in\n",
    "    increasing power form, contrary to matlab's convention. For instance, here \n",
    "    a(0) is the coefficient of L^0.\n",
    "    '''\n",
    "    \n",
    "    if(mu.shape[0] == 1):\n",
    "        mu = mu.conj().transpose()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Construct various utility matrices\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # N is the Vandermonde that will be used to evaluate the numerator.\n",
    "    NM = np.zeros((len(mu),Kb+1))\n",
    "\n",
    "    NM[:,0] = 1\n",
    "\n",
    "    for k in range(1, Kb+1):\n",
    "        NM[:,k] = NM[:,k-1] * mu\n",
    "\n",
    "    # M is the Vandermonde that will be used to evaluate the denominator.\n",
    "    MM = np.zeros((len(mu), Ka))\n",
    "\n",
    "    MM[:,0] = mu  \n",
    "\n",
    "    for k in range(1, Ka):\n",
    "        MM[:,k] = MM[:,k-1] * mu\n",
    "\n",
    "    V = np.zeros((np.size(mu),Ka))\n",
    "\n",
    "    for k in range(0, Ka):\n",
    "        V[:,k] = mu**k\n",
    "  \n",
    "    '''n = np.size(mu)\n",
    "    C1 = np.zeros((n,n*Ka))\n",
    "    for k in range(1, Ka):\n",
    "        C1[(k-1)*n+1:(k-1)*n+n, (k-1)*n+1:(k-1)*n+n] = np.diag(mu**k)\n",
    "    '''\n",
    "\n",
    "    ia = Variable(shape=(Ka,1))\n",
    "    ib = Variable(shape=(Kb+1,1))\n",
    "    \n",
    "    '''print(np.diag(response(mu)).shape)\n",
    "    print(MM.shape)\n",
    "    print(ia.shape)\n",
    "    print(ib.shape)\n",
    "    \n",
    "    print(type(np.diag(response(mu))))\n",
    "    print(type(MM))\n",
    "    print(type(ia))\n",
    "    print(type(ib))'''\n",
    "    \n",
    "    reshape_mu = response(mu).reshape(response(mu).shape[0], 1)\n",
    "    \n",
    "    objective = Minimize(norm(reshape_mu + np.diag(response(mu))@MM@ia - NM@ib))\n",
    "    constraints = [max(abs(V@ia)) <= radius]\n",
    "    \n",
    "    prob = Problem(objective, constraints)\n",
    "    result = prob.solve(verbose=True)\n",
    "    \n",
    "    a = ia.value\n",
    "    b = ib.value\n",
    "    \n",
    "    # least-squares (again to find b)\n",
    "    #B = np.fliplr(np.vander(mu))    \n",
    "    #b = lsq_linear(np.divide(B[:,0:Kb+1], np.matlib.repmat(B[:,1:Ka+1]@a,1,Kb+1)), reshape_mu)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Optimize it with newton's iteration\n",
    "    # -------------------------------------------------------------------------\n",
    "    '''if(radius >= 1):\n",
    "        a, b = dlsqrat(mu, response(mu), Kb, Ka, a[2:end])\n",
    "        a = concatenate((1,a))\n",
    "    '''    \n",
    "    # this is the achieved response\n",
    "    rARMA = np.polyval(np.flipud(b),mu)/np.polyval(np.flipud(a), mu)\n",
    "\n",
    "    # error\n",
    "    error = norm(rARMA - response(mu))/norm(mu)\n",
    "    \n",
    "    return b, a, rARMA, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlsqrat(t,y,p,q,alpha):\n",
    "    \n",
    "    # Set the convergence tolerance.\n",
    "    TOLERANCE = 10**(-12)\n",
    "    \n",
    "    # N is the Vandermonde that will be used to evaluate the numerator.\n",
    "    N = np.zeros((len(t),p+1))\n",
    "    N[:,1] = np.ones((len(t),1))\n",
    "    for k in range(2, p+1):\n",
    "        N[:,k] = N[:,k-1]*t\n",
    "    \n",
    "    # M is the Vandermonde that will be used to evaluate the denominator.\n",
    "    M = np.zeros((len(t),q))\n",
    "    M[:,1] = t\n",
    "    for k in range(2, q):\n",
    "        M[:,k] = M[:,k-1]*t\n",
    "    \n",
    "    # If we are not given an initial guess, then generate one.\n",
    "    if(nargin < 5):\n",
    "        tmp_pade = np.linalg.solve((N - np.diag(y)*M),y)\n",
    "        alpha = tmp_pade[p+2:end]\n",
    "    \n",
    "    # Construct the model matrix and compute ancillary quantities.\n",
    "    update(alpha)\n",
    "    \n",
    "    # iterate\n",
    "    for iterate in range(1, 100):\n",
    "\n",
    "        # Update the error.\n",
    "        old_err = err\n",
    "\n",
    "        # Compute the Jacobian and the Hessian.\n",
    "        Tmp1 = np.diag(Py*D) * M\n",
    "        Tmp2 = Q.conj().transpose() * np.diag((Py-r)*D) * M\n",
    "        J = Tmp1 - Q * Tmp2\n",
    "        H = M.conj().transpose() * np.diag((Py-2*r)*D) * Tmp1 - Tmp2.conj().transpose() * Tmp2\n",
    "\n",
    "        # Compute the gradient.\n",
    "        gradient = J.conj().transpose()*r\n",
    "\n",
    "        # Compute the Cholesky factorization of H.\n",
    "        R, not_PD = np.linalg.cholesky(H)\n",
    "\n",
    "        # If H is not positive definite then regularize and factor\n",
    "        if(not_PD):\n",
    "            R = np.linalg.cholesky(H - 2.5*(linalg.eig(H))*eye(q)).min(0)\n",
    "\n",
    "        #Compute the Newton step.\n",
    "        delta = np.linalg.lstsq(-R,(np.linalg.lstsq(R.conj().transpose(),gradient)))\n",
    "\n",
    "        # Use stepsize control to take a step.\n",
    "        step_control()\n",
    "\n",
    "        # Convergence testingy\n",
    "        if(err > old_err):\n",
    "            print('Failed to find descending step length.')\n",
    "            break;\n",
    "        else:\n",
    "            alpha = new_alpha\n",
    "            rel_err = np.abs(old_err - err)/old_err\n",
    "            if(rel_err <= TOLERANCE):\n",
    "                break\n",
    "                \n",
    "    # Compute the coefficients of the numerator.\n",
    "    c = np.linalg.lstsq((np.diag(D)*N),y)\n",
    "\n",
    "    # Generate an error message if the algorithm failed to converge.\n",
    "    if(rel_err > TOLERANCE):\n",
    "        print('Algorithm did not converge.')\n",
    "        \n",
    "    def update(alpha):\n",
    "        # This function updates the model matrix and computes ancillary quantities.\n",
    "        \n",
    "        # Compute the denominator.\n",
    "        D = 1/(1+M*alpha)\n",
    "        \n",
    "        # Compute the QR factorization of A = D*N\n",
    "        Q, R = np.linalg.qr(np.diag(D)*N,0)\n",
    "        \n",
    "        # Compute the projection of y onto the range of A.\n",
    "        Py = Q*(Q.conj().transpose()*y)\n",
    "        \n",
    "        # Compute the residual.y\n",
    "        r = y - Py\n",
    "        \n",
    "        # Compute the current squared error.\n",
    "        err = r.conj().transpose()*r\n",
    "        \n",
    "    def step_control():\n",
    "        # This function implements stepsize control using a simple backtracking scheme from Dennis & Schnabel.\n",
    "        \n",
    "        # Try taking a full step.\n",
    "        new_alpha = alpha + delta\n",
    "        \n",
    "        # Update the model.\n",
    "        update(new_alpha)\n",
    "        \n",
    "        # If a full step does not sufficiently reduce the error then we\n",
    "        # use a backtracking line-search method for step-size control.\n",
    "        # This involves minimizing a function f(lambda) that interpolates the\n",
    "        # computed error (and its derivatives) at different values of lambda.\n",
    "        f0 = old_err\n",
    "        fprime = gradient.conj().transpose()*delta\n",
    "        steptol = f0 + 0.0001*fprime\n",
    "        if(err > steptol):\n",
    "            errs[1] = err\n",
    "            lams[1] = 1\n",
    "            \n",
    "            # refinement is necessary.\n",
    "            # We'll need this if further\n",
    "            # We start with a quadratic model at f(0), f'(0), and f(1)\n",
    "            # and will take the larger of the computed step or 1/10.\n",
    "            lambdas = np.array([-fprime/(2*(err - f0 - fprime)), 0.1]).max(0)\n",
    "            new_alpha = alpha + lambdas*delta\n",
    "            \n",
    "            # Update the model matrix and compute ancillary quantities.\n",
    "            update(new_alpha);\n",
    "            \n",
    "            # If this doesn't work then we loop with a cubic model at f(0),\n",
    "            # f'(0), f(lambda), and f(lam2) where the last two are errors at\n",
    "            # the last two lambda that were tried.\n",
    "            steptol = f0 + 0.0001*fprime*lambdas\n",
    "            while(err > steptol):\n",
    "                # Push the current lambda and error to the top of the lams and errs\n",
    "                # stacks.\n",
    "                lams = np.concatenate((lambdas,lams[1]), axis=0)\n",
    "                errs = np.concatenate((err, errs[1]), axis=0)\n",
    "                rhs = (errs - fprime*lams - np.vstack((f0,f0)))/(lams*lams)\n",
    "                ab = np.linalg.lstsq(np.array([lams, np.vstack((1,1))]), rhs)\n",
    "                lambdas = (-ab[2]+sqrt(ab[2]*ab[2] - 3*ab[1]*fprime))/(3*ab[1]);\n",
    "                # It is still important to make certain that the new lambdas\n",
    "                # progresses quickly but not too quickly. So if lambdas is less\n",
    "                # than lam2/10 we just use lam2/10, and if it is larger than\n",
    "                # lam2/2 then we use lam2/2.\n",
    "                if(lambdas < lams[1]/10):\n",
    "                    lambdas = lams[1]/10\n",
    "                \n",
    "                if(lambdas > lams[1]/2):\n",
    "                    lambdas = lams[1]/2\n",
    "                \n",
    "                new_alpha = alpha + lambdas*delta\n",
    "                # Update the model matrix and compute ancillary quantities.\n",
    "                update(new_alpha)\n",
    "                steptol = f0 + 0.0001*fprime*lambdas\n",
    "    \n",
    "    return alpha, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
      "\n",
      "ECOS 2.0.7 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +0.000e+00  -7.500e-01  +1e+03  8e-01  9e-01  1e+00  1e+00    ---    ---    1  1  - |  -  - \n",
      " 1  -1.378e+00  -1.663e+00  +5e+02  3e-01  4e-01  5e-01  6e-01  0.6046  7e-02   2  1  1 |  0  0\n",
      " 2  -6.385e+01  -6.417e+01  +5e+02  3e-01  2e-01  2e-01  6e-01  0.2589  9e-01   2  2  3 |  0  0\n",
      " 3  -1.161e-01  -9.107e-02  +7e+01  2e-02  7e-03  6e-02  8e-02  0.9890  1e-01   3  2  2 |  0  0\n",
      " 4  +1.392e+00  +1.395e+00  +9e+00  2e-03  8e-04  8e-03  1e-02  0.8642  3e-03   3  3  3 |  0  0\n",
      " 5  +1.207e+00  +1.212e+00  +5e+00  1e-03  1e-03  9e-03  5e-03  0.6590  2e-01   4  3  3 |  0  0\n",
      " 6  +1.243e+00  +1.243e+00  +1e+00  3e-04  2e-04  2e-03  1e-03  0.8486  8e-02   4  3  4 |  0  0\n",
      " 7  +1.217e+00  +1.217e+00  +1e-01  3e-05  2e-05  1e-04  1e-04  0.9522  6e-02   4  3  3 |  0  0\n",
      " 8  +1.219e+00  +1.219e+00  +1e-03  3e-07  3e-07  1e-06  1e-06  0.9890  1e-04   4  3  3 |  0  0\n",
      " 9  +1.219e+00  +1.219e+00  +1e-05  3e-09  3e-09  1e-08  1e-08  0.9890  1e-04   4  2  2 |  0  0\n",
      "10  +1.219e+00  +1.219e+00  +1e-07  4e-11  3e-11  1e-10  2e-10  0.9890  1e-04   4  2  2 |  0  0\n",
      "11  +1.219e+00  +1.219e+00  +2e-09  4e-12  4e-13  2e-12  2e-12  0.9883  1e-04   4  2  2 |  0  0\n",
      "\n",
      "OPTIMAL (within feastol=3.6e-12, reltol=1.4e-09, abstol=1.7e-09).\n",
      "Runtime: 0.006387 seconds.\n",
      "\n",
      "[[ 2.688e-02]\n",
      " [-2.838e-01]\n",
      " [-2.046e+00]\n",
      " [ 3.413e+00]\n",
      " [ 2.019e+01]\n",
      " [-4.597e+00]\n",
      " [-5.244e+01]\n",
      " [-1.227e-01]\n",
      " [ 5.405e+01]\n",
      " [ 1.761e+00]\n",
      " [-1.969e+01]]\n",
      "[[-7.500e-01]\n",
      " [-2.842e-10]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2708, 3)\n",
      "(2708,)\n",
      "[-2.258e+08  2.014e+10  3.389e+13 ... -4.843e+15 -1.986e+11  2.216e+09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/u6537967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/home/users/u6537967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "X, A, Y = load_data(dataset='cora')\n",
    "A = np.array(A.todense())\n",
    "\n",
    "# Identity matrix for self loop\n",
    "I = np.matrix(np.eye(A.shape[0]))\n",
    "A_hat = A + I\n",
    "\n",
    "# Degree matrix\n",
    "D_hat = np.array(np.sum(A_hat, axis=0))[0]\n",
    "D_hat = np.matrix(np.diag(D_hat))\n",
    "\n",
    "#Laplacian matrix\n",
    "L = D_hat - A_hat\n",
    "\n",
    "#x is a signal with N number of nodes, L: laplacian\n",
    "lambda_cut = 0.5;\n",
    "\n",
    "def step(x, a):\n",
    "    for index in range(len(x)):\n",
    "        if(x[index] >= a):\n",
    "            x[index] = float(1)\n",
    "        else:\n",
    "            x[index] = float(0)\n",
    "    return x\n",
    "    \n",
    "response = lambda x: step(x, lmax(L)/2 - lambda_cut)\n",
    "\n",
    "# Since the eigenvalues might change, sample eigenvalue domain uniformly\n",
    "l = np.linspace(0, lmax(L), 300)\n",
    "mu = lmax(L)/2-l\n",
    "\n",
    "#For stability, we will work with a shifted version of the Laplacian\n",
    "M = sp.csr_matrix(L) #0.5*lmax(L)*np.matrix(np.eye(L.shape[0])) - L\n",
    "\n",
    "#AR filter order (decrease radius for larger values)\n",
    "Ka = 2\n",
    "\n",
    "#MA filter order\n",
    "Kb = 10\n",
    "\n",
    "#for speed make small, for accuracy increase. Should be below 1 if the distributed implementation is used. \n",
    "#With the (faster) conj. gradient implementation, any radius is allowed.\n",
    "radius = 0.75\n",
    "\n",
    "b, a, rARMA, error = agsp_design_ARMA(mu, response, Kb, Ka, radius)\n",
    "\n",
    "print(b)\n",
    "print(a)\n",
    "\n",
    "'''#Compute the filter.\n",
    "#denominator\n",
    "alpha_n = np.dot(a[0], M)\n",
    "for index in range(1, len(a)):\n",
    "    alpha_n += np.dot(a[index], (M**index))\n",
    "\n",
    "polynomial_of_denominator = alpha_n\n",
    "\n",
    "#numerator\n",
    "beta_n = 0\n",
    "for index in range(len(b)):\n",
    "    #beta_n += np.dot(b[index], (M**index))\n",
    "    print(b[index])\n",
    "\n",
    "polynomial_of_numerator = beta_n\n",
    "\n",
    "arma_filter = (polynomial_of_denominator**-1)*(polynomial_of_numerator)\n",
    "\n",
    "print(arma_filter)'''\n",
    "\n",
    "# Normalize X\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "X = np.array(X)\n",
    "\n",
    "T = (np.array([5*np.array([Ka,Kb]).max(0), 30])).max(0)\n",
    "\n",
    "y = agsp_filter_ARMA(M, b, a, X[:,0], T)\n",
    "\n",
    "y_dash = agsp_filter_ARMA_cgrad(M, b, a, X[:,0], T)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_dash.shape)\n",
    "\n",
    "print(y_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
